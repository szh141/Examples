{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/szh141/Examples/blob/main/Patient_data_preprocessing_for_ML.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://levelup.gitconnected.com/mastering-feature-engineering-process-in-data-science-6897ba5a2d7a\n",
        "\n"
      ],
      "metadata": {
        "id": "KCgk8Qfkozuq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Conclusion\n",
        "\n",
        "Throughout our analysis, we started with the goal of optimizing a dataset for a machine learning model focused on predicting the variable “Developed_Pneumonia”. We began by identifying and removing redundant or irrelevant variables from the dataset, ensuring a more streamlined and effective data base.\n",
        "\n",
        "Next, we implemented encoding techniques to transform categorical variables into numerical formats, essential for predictive analysis.\n",
        "\n",
        "The crucial step was the selection of relevant features. We used the RandomForestClassifier, a powerful tool for assessing variable importance. With this, we identified the most significant features for predicting pneumonia, allowing us to focus on data that truly influences the outcome. The selection process was optimized and streamlined for efficiency, resulting in clear and straightforward code.\n",
        "\n",
        "Finally, we created an optimized dataset, containing only the most relevant features and the target variable, and saved it in CSV format.\n",
        "\n",
        "This refined dataset is now ready to be applied in predictive models, potentially improving accuracy and performance. In summary, our work here established a solid foundation for building an efficient and reliable machine learning model for pneumonia prediction."
      ],
      "metadata": {
        "id": "hp-pabm7pqwH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0PNwU349D4f0"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the dataset\n",
        "file_path = 'dados_pacientes.csv'\n",
        "dataset = pd.read_csv(file_path)\n",
        "\n",
        "# Display the first few rows of the dataset to understand its structure\n",
        "dataset.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculating statistical summary for numerical variables\n",
        "numerical_summary = dataset.describe()\n",
        "\n",
        "# Calculating summary for categorical variables\n",
        "categorical_summary = dataset.describe(include=['object'])\n",
        "\n",
        "numerical_summary, categorical_summary\n"
      ],
      "metadata": {
        "id": "uqGvt0dOIIH8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Calculando a correlação entre Peso e Altura\n",
        "correlation = dataset['Peso'].corr(dataset['Altura'])\n",
        "\n",
        "# Criando um scatter plot para visualizar a relação entre Peso e Altura\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.scatterplot(data=dataset, x='Altura', y='Peso')\n",
        "plt.title('Relazione tra Peso e Altezza dei Pazienti')\n",
        "plt.xlabel('Altezza (cm)')\n",
        "plt.ylabel('Peso (kg)')\n",
        "plt.grid(True)\n",
        "\n",
        "# Mostrando o coeficiente de correlação\n",
        "plt.figtext(0.5, 0.01, f\"Coefficiente di Correlazione: {correlation:.2f}\", ha=\"center\", fontsize=12, bbox={\"facecolor\":\"orange\", \"alpha\":0.5, \"pad\":5})\n",
        "\n",
        "plt.show()\n",
        "\n",
        "correlation\n",
        "\n",
        "# 0.365"
      ],
      "metadata": {
        "id": "XJI3NtqmM2g3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calcolo dell'Indice di Massa Corporea (IMC)\n",
        "dataset['IMC'] = (dataset['Peso'] / (dataset['Altura'] / 100) ** 2).round(0)\n",
        "\n",
        "# Visualizzando le prime righe del dataset per verificare la nuova colonna\n",
        "dataset.head()\n"
      ],
      "metadata": {
        "id": "6e1WZeCB5W26"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    # Calculando o IMC\n",
        "    dataset['IMC'] = (dataset['Peso'] / ((dataset['Altura'] / 100) ** 2)).round(0)\n",
        "\n",
        "    # Criando o gráfico de distribuição do IMC\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    sns.histplot(dataset['IMC'], bins=20, kde=True)\n",
        "    plt.title('Distribuzione dell\\'Indice di Massa Corporea (IMC) dei Pazienti')\n",
        "    plt.xlabel('IMC')\n",
        "    plt.ylabel('Frequenza')\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "except Exception as e:\n",
        "    print(f\"Ocorreu um erro: {e}\")\n"
      ],
      "metadata": {
        "id": "H146Sn0X6Qon"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Removendo as variáveis Peso e Altura do dataset\n",
        "dataset = dataset.drop(['Peso', 'Altura'], axis=1)\n",
        "\n",
        "# Exibindo as primeiras linhas do dataset para verificar as alterações\n",
        "dataset.head()\n"
      ],
      "metadata": {
        "id": "b7oZ7OcA7UZT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Criando uma nova coluna 'Codigo_Hist_Paciente' com os primeiros 5 dígitos da coluna 'ID_Paciente'\n",
        "dataset['Codigo_Hist_Paciente'] = dataset['ID_Paciente'].str[:5]\n",
        "\n",
        "# Exibindo as primeiras linhas do dataset para verificar a nova coluna\n",
        "dataset.head()\n"
      ],
      "metadata": {
        "id": "HBA27eOa7U3C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Verificando o total de valores únicos na coluna 'Codigo_Hist_Paciente'\n",
        "try:\n",
        "    unique_codes = dataset['Codigo_Hist_Paciente'].nunique()\n",
        "    unique_codes\n",
        "except Exception as e:\n",
        "    unique_codes = \"Erro ao calcular: \" + str(e)\n",
        "\n",
        "unique_codes\n"
      ],
      "metadata": {
        "id": "sqOWRXpZCMxt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Criando uma nova coluna 'faixa_etaria' com base na coluna 'Idade'\n",
        "# Definindo as faixas etárias\n",
        "bins = [0, 20, 30, 45, 50, 100]\n",
        "labels = ['0-20', '20-30', '30-40', '40-50', '50+']\n",
        "\n",
        "# Usando a função cut do pandas para criar as categorias\n",
        "dataset['faixa_etaria'] = pd.cut(dataset['Idade'], bins=bins, labels=labels, right=False)\n",
        "\n",
        "# Exibindo as primeiras linhas do dataset para verificar a nova coluna\n",
        "dataset.head()\n"
      ],
      "metadata": {
        "id": "QcBmHod6CNbt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Relação entre ser fumante e o desenvolvimento de pneumonia\n",
        "relation_smoking_pneumonia = dataset[['Fumante', 'Desenvolveu_Pneumonia']].groupby(['Fumante']).agg('count')\n",
        "relation_smoking_pneumonia"
      ],
      "metadata": {
        "id": "l6ntuYMhWYwB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "from matplotlib import pyplot as plt\n",
        "relation_smoking_pneumonia['Desenvolveu_Pneumonia'].plot(kind='hist', bins=20, title='Desenvolveu_Pneumonia')\n",
        "plt.gca().spines[['top', 'right',]].set_visible(False)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "IcRNY5Vimvwj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Criando uma tabela de contingência entre as variáveis 'Fumante' e 'Desenvolveu_Pneumonia'\n",
        "contingency_table = pd.crosstab(dataset['Fumante'], dataset['Desenvolveu_Pneumonia'])\n",
        "\n",
        "contingency_table\n"
      ],
      "metadata": {
        "id": "dPceGqDoWZRC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.stats import chi2_contingency\n",
        "# Creando la tabella di contingenza per i dati di fumatori e sviluppo della polmonite\n",
        "tabella_contingenza = [[278, 268], [220, 234]]\n",
        "\n",
        "# Esecuzione del test del Chi-quadrato\n",
        "chi2, p, dof, expected = chi2_contingency(tabella_contingenza)\n",
        "chi2, p"
      ],
      "metadata": {
        "id": "mjZ0gRQhYj5V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Definindo as faixas\n",
        "bins = [-1, 2, 4, dataset['Numero_Filhos'].max()]\n",
        "\n",
        "# Definindo os rotulos para as faixas\n",
        "labels = ['0-2', '3-4', '5+']\n",
        "\n",
        "# Criando a nova coluna categórica\n",
        "dataset['Faixa_Filhos'] = pd.cut(dataset['Numero_Filhos'], bins=bins, labels=labels, right=True)\n",
        "\n",
        "# Exibindo as primeiras linhas do dataset para verificar a nova coluna\n",
        "dataset.head()"
      ],
      "metadata": {
        "id": "4Bb5iVh2mqcH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Colunas a serem removidas\n",
        "colunas_para_remover = ['ID_Paciente', 'Data_Nascimento', 'Idade', 'Numero_Filhos', 'Codigo_Hist_Paciente']\n",
        "\n",
        "# Removendo as colunas\n",
        "clean_dataset = dataset.drop(columns=colunas_para_remover)\n",
        "clean_dataset.head()"
      ],
      "metadata": {
        "id": "4h5vpUr0mq05"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Lista para armazenar os resultados\n",
        "results = []\n",
        "\n",
        "# Colunas categoricas\n",
        "categorical_columns = clean_dataset.select_dtypes(include=[pd.np.object]).columns.tolist()\n",
        "categorical_columns.remove('Desenvolveu_Pneumonia')\n",
        "\n",
        "# Loop por todas as colunas categoricas\n",
        "for col in categorical_columns:\n",
        "\n",
        "# Criando a tabela de contingencia\n",
        "  contingency_table = pd.crosstab(clean_dataset[col], clean_dataset['Desenvolveu_Pneumonia'])\n",
        "\n",
        "  # Realizando o teste do qui-quadrado\n",
        "  chi2, p_value, dof, expected = chi2_contingency(contingency_table)\n",
        "\n",
        "  # Adicionando os resultados a lista\n",
        "  results.append({'Variable': col, 'p-value': p_value})\n",
        "\n",
        "# Convertendo os resultados em um DataFrame para uma visualizacao mais facil\n",
        "results_df = pd.DataFrame(results)\n",
        "results_df"
      ],
      "metadata": {
        "id": "EivXHQZAr9Jg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
        "\n",
        "# Criando os codificadores\n",
        "le = LabelEncoder()\n",
        "ohe = OneHotEncoder(drop = 'first', sparse = False)\n",
        "\n",
        "# Codificando a variavel alvo\n",
        "clean_dataset['Desenvolveu_Pneumonia'] = le.fit_transform(clean_dataset['Desenvolveu_Pneumonia'])\n",
        "\n",
        "# Lista para armazenar as novas colunas categoricas co\n",
        "new_categorical_cols = []\n",
        "\n",
        "# Loop por todas as colunas categoricas\n",
        "for col in categorical_columns:\n",
        "\n",
        "  # Codificando a coluna\n",
        "  encoded_cols = ohe.fit_transform(clean_dataset[[col]])\n",
        "\n",
        "  # Transformando o resultado em um DataFrame e Adicionar\n",
        "  encoded_cols_df = pd.DataFrame(encoded_cols, columns = [f\"{col}_{category}\" for category in ohe.categories_[0][1:]])\n",
        "  # Adicionando o DataFrame resultante a lista\n",
        "  new_categorical_cols.append(encoded_cols_df)\n",
        "\n",
        "# Concatenando todos os DataFrames da lista\n",
        "new_categorical_cols_clean_dataset= pd.concat(new_categorical_cols, axis=1)\n",
        "\n",
        "# Removendo as colunas categoricas originais do DataFrame\n",
        "cleand_dataset = clean_dataset.drop(categorical_columns, axis=1)\n",
        "\n",
        "# Adicionando as novas colunas categoricas codificada\n",
        "df = pd.concat([clean_dataset, new_categorical_cols_clean_dataset], axis=1)\n",
        "\n",
        "# Primeiras linhas\n",
        "df.head()\n"
      ],
      "metadata": {
        "id": "2XfMeQfhxaw6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Aplicando One-Hot Encoding para 'faixa_etaria' e 'Faixa_Filhos' usando pd.get_dummies()\n",
        "df = pd.get_dummies(df, columns=['faixa_etaria', 'Faixa_Filhos'], drop_first=True)\n",
        "\n",
        "# Visualizando as primeiras linhas do DataFrame atualizado\n",
        "print(df.head())\n"
      ],
      "metadata": {
        "id": "h1lLAjaWvpAH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.feature_selection import SelectFromModel\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "# Carregar o DataFrame\n",
        "# df = pd.read_csv('seu_dataset.csv')\n",
        "\n",
        "# Identificar colunas categóricas\n",
        "categorical_cols = df.select_dtypes(include=['object', 'category']).columns\n",
        "\n",
        "# Aplicar One-Hot Encoding para variáveis categóricas\n",
        "df_encoded = pd.get_dummies(df, columns=categorical_cols, drop_first=True)\n",
        "\n",
        "# Separar características e variável alvo\n",
        "X = df_encoded.drop('Desenvolveu_Pneumonia', axis=1)\n",
        "y = df_encoded['Desenvolveu_Pneumonia']\n",
        "\n",
        "# Criar e treinar o modelo Random Forest\n",
        "model = RandomForestClassifier()\n",
        "model.fit(X, y)\n",
        "\n",
        "# Selecionar características com base na importância\n",
        "selector = SelectFromModel(model, threshold='median')\n",
        "X_selected = selector.fit_transform(X, y)\n",
        "\n",
        "# Obter os nomes das características selecionadas\n",
        "selected_features = X.columns[selector.get_support()]\n",
        "\n",
        "print(\"Características selecionadas:\", selected_features)\n"
      ],
      "metadata": {
        "id": "yfVNQyFOypyS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.feature_selection import SelectFromModel\n",
        "\n",
        "# Aplicar One-Hot Encoding e separar as características e a variável alvo\n",
        "df_encoded = pd.get_dummies(df, drop_first=True)\n",
        "X = df_encoded.drop('Desenvolveu_Pneumonia', axis=1)\n",
        "y = df['Desenvolveu_Pneumonia']\n",
        "\n",
        "# Treinar o modelo Random Forest e selecionar características importantes\n",
        "model = RandomForestClassifier().fit(X, y)\n",
        "selected_features = X.columns[SelectFromModel(model, threshold='median').fit(X, y).get_support()]\n",
        "\n",
        "# Criar um novo DataFrame com as características selecionadas e a variável alvo\n",
        "df_final = df_encoded[selected_features].join(df['Desenvolveu_Pneumonia'])\n",
        "\n",
        "# Salvar o novo DataFrame em um arquivo CSV\n",
        "df_final.to_csv('dataset_otimizado.csv', index=False)\n",
        "\n",
        "print(\"Dataset otimizado salvo como 'dataset_otimizado.csv'.\")\n"
      ],
      "metadata": {
        "id": "JeLudxql3EsW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}